{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UCD Data Soc: Text Analysis Workshop",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPuOldsBVqdn"
      },
      "source": [
        "# Text Analysis Workshop\n",
        "\n",
        "We'll be working with two datasets in this workshop. One, `plot_summaries`, contains film summaries extracted from Wikipedia articles while the other dataset, `metadata`, contains further information about each movie such as Title, Release Year and Genres.\n",
        "\n",
        "\n",
        "We'll go through manipulating and plotting this data with python first. Then we'll cover some basic text analysis of the summaries. Finally we'll train a classifier that when given a film summary can predict its genre. \n",
        "\n",
        "For an introduction to how this web tool (Google Colab Notebook) works go [here](https://colab.research.google.com/notebooks/intro.ipynb). Basically it lets us write and run code online without downloading anything to our computers. \n",
        "\n",
        "We'll be writing code in python throughout but you don't need to have a lot of experience with python to follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhRC0lGU_KNb"
      },
      "source": [
        "# Run the following cells in order to set up the data and code libraries we will be using in the workshop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahFV3LOf_HXQ"
      },
      "source": [
        "#@title Download data {display-mode: \"form\"}\n",
        "#@markdown [README for original data](http://www.cs.cmu.edu/~ark/personas/data/README.txt)\n",
        "#@markdown Press the run button to the left to download the movie dataset we will be using in this workshop.\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "movie_metadata_link = \"https://www.dropbox.com/s/75no5lozv4og3g9/movie_metadata.csv?dl=0\"\n",
        "movie_plots_link = \"https://www.dropbox.com/s/71rlzwy4xvaq8nh/plot_summaries.csv?dl=0\"\n",
        "!wget -O movie_metadata.csv {movie_metadata_link}\n",
        "!wget -O plot_summaries.csv {movie_plots_link}\n",
        "\n",
        "# update library versions\n",
        "!pip install -U seaborn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1jQjTwzvPCH"
      },
      "source": [
        "## Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WskmuZh_I9R"
      },
      "source": [
        "import pandas as pd # pandas library for loading and manipulating data\n",
        "import numpy as np\n",
        "from ast import literal_eval # function for loading lists correctly\n",
        "from sklearn.preprocessing import MultiLabelBinarizer # method for turning a list feature into columns\n",
        "\n",
        "from collections import Counter # object for counting frequency of words\n",
        "import nltk # nltk library for text analysis functions\n",
        "from nltk.corpus import stopwords # list of stopwords from nltk\n",
        "from nltk.stem import WordNetLemmatizer # function for \"lemmatizing\" words\n",
        "import string # library of functions for dealing with strings\n",
        "\n",
        "# plotting libraries\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVXl0FlGA7K-"
      },
      "source": [
        "# datasets from the nltk library \n",
        "# stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "# lemmatizer \n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gksZNLAA98X"
      },
      "source": [
        "# some settings for display of data in notebook\n",
        "pd.options.display.max_rows = 1000\n",
        "pd.options.display.max_colwidth = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcHLZ7eJBSdy"
      },
      "source": [
        "# Load Movie Data\n",
        "\n",
        "We use the [__pandas library__]() to load the dataset into a useable format in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyXmFsg9B0UM"
      },
      "source": [
        "plot_summaries = pd.read_csv(\"plot_summaries.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvbjwDn0BziS"
      },
      "source": [
        "metadata = pd.read_csv(\"movie_metadata.csv\",\n",
        "                       converters={\"languages\": literal_eval,\n",
        "                                   \"countries\": literal_eval,\n",
        "                                   \"genres\": literal_eval},\n",
        "                       # this loads the lists as lists instead of strings\n",
        "                       encoding=\"utf-8\") \n",
        "                       # this loads the text in the correct encoding format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvcNkBmSCJBs"
      },
      "source": [
        "## What does this dataset contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpJ55DS8CIE_"
      },
      "source": [
        "# this will show us the column names and data types of the metadata dataframe\n",
        "metadata.dtypes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcy7xFjkUIrD"
      },
      "source": [
        "# do the same for the plot_summaries dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFfenlyXURW8"
      },
      "source": [
        "# number of rows in the summaries \"dataframe\"\n",
        "plot_summaries.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTfiIms5UW8A"
      },
      "source": [
        "# we expect each row to have a unique wikiId\n",
        "plot_summaries.wikiId.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZkzmblUZ-D"
      },
      "source": [
        "# how many rows does the metadata dataframe have?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpnj6vGnUefR"
      },
      "source": [
        "# does each metadata row have a unique wikiId value?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Xmob75Uifn"
      },
      "source": [
        "# let's look at the data for one movie - pick a movie title\n",
        "metadata[metadata.name==\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3spu8v2U1lR"
      },
      "source": [
        "## Exploring the genres feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS5yOLP8Uq5V"
      },
      "source": [
        "# This function will take the genres feature and transform it from a list into its own dataframe\n",
        "def list_into_df(df, list_col):\n",
        "  mlb = MultiLabelBinarizer()\n",
        "  new_cols = pd.DataFrame(mlb.fit_transform(df[list_col]), columns=mlb.classes_, index=df.index)\n",
        "  return new_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uMc2R4bVLH2"
      },
      "source": [
        "# create the \"genres belonging to each movie\" dataframe\n",
        "genre_df = list_into_df(metadata, 'genres')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wajTpEOJVmAQ"
      },
      "source": [
        "# which genres are the most common?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApHaD31IWXHa"
      },
      "source": [
        "# which genres are the least common?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aupEI0CZG71"
      },
      "source": [
        "# what proportion of films belong to each genre "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLsGD_4OWZWC"
      },
      "source": [
        "# how many genres does each movie have? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4tZRuElWfx9"
      },
      "source": [
        "# what is the average number of genres that each movie has?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUCDkAl5XMIc"
      },
      "source": [
        "# plot the number of genres per movie in a `barplot`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBiNjiNEXdPl"
      },
      "source": [
        "### Let's focus on one genre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKtQjWSXfno"
      },
      "source": [
        "# create a feature that is True if a row is a 'Romance Film' and False if it is not."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrwGmtYCY7-X"
      },
      "source": [
        "# what proportion of films are classified as 'Romance Film'?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkXfXcQ-Y_8L"
      },
      "source": [
        "# how has the proportion of films that are 'Romance Film' genre changed over time?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBs-ZEWnZVJO"
      },
      "source": [
        "# plot the proportion of 'Romance Film' over time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDYY-4BbZbTF"
      },
      "source": [
        "# Text Analysis\n",
        "Let's move on the analysing the plot summary data directly.\n",
        "\n",
        "\n",
        "You may have noticed already that there are more rows in the metadata dataset than there are in the plot_summaries dataset. For this portion we only care about the movies that are in both datasets. So let's start by joining these two datasets together and only keeping movies that are in both. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3oqhZHyZayu"
      },
      "source": [
        "# how many plot_summaries wikiId values are also in the metadata dataframe?\n",
        "metadata.wikiId.isin(plot_summaries.wikiId)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQEW-dy3aJHz"
      },
      "source": [
        "# let's merge the dataframes using the wikiId to join them."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqFg9ntcaYM8"
      },
      "source": [
        "## What are the most frequently occuring words in all the plot_summaries?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_4ApVM0aX8O"
      },
      "source": [
        "# join all the plot summaries into one string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgCjmUNajHO"
      },
      "source": [
        "# turn the long string into a list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjSpZKxBalpL"
      },
      "source": [
        "# count the occurance of each word in the list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jgLDfOgasuw"
      },
      "source": [
        "## Removing 'uninteresting' words.\n",
        "Some of these words occur very frequently but don't give us much information about the content of the summary. e.g. \"and\" is a very common word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBZxm0y_a4_g"
      },
      "source": [
        "# create a list of uninteresting words and filter our original list of words."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLYWwBHrbCA5"
      },
      "source": [
        "## Clean messy data\n",
        "This approach of splitting a long string into words leaves us with some \"words\" that don't exactly match what we're looking for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_T0Mv80a_z9"
      },
      "source": [
        "# this creates a table of replacements for any punctuation\n",
        "punc_table = str.maketrans({p: \"\" for p in string.punctuation})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McnuPONHba8r"
      },
      "source": [
        "# remove all the punctuation from the list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYCR6spKbesD"
      },
      "source": [
        "# remove all empty strings from the list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-CbNsOjbmRR"
      },
      "source": [
        "## Tidying verbs and other forms of words.\n",
        "Some words such as verbs like talk can appear in multiple forms in text. e.g. talk, talking, talked, ...\n",
        "\n",
        "\n",
        "However their meaning is roughly the same for our purpose. We can try turn all versions of these verbs into one form so our most common count of words is more accurate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctprDSqEcOnS"
      },
      "source": [
        "wnl = nltk.WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO-5iuKpcZV0"
      },
      "source": [
        "wnl.lemmatize(\"talking\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmneGxpcpx8"
      },
      "source": [
        "# lets lemmatize all of the words in our list of tokens."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsVivBdAcze6"
      },
      "source": [
        "## Plotting word frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmTqAe1cych"
      },
      "source": [
        "# plot the top 10 most frequent words "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3uYNLX2fH1A"
      },
      "source": [
        "# generate a 'wordcloud'\n",
        "DICT_OF_WORD_FREQUENCIES = \n",
        "wordcloud = WordCloud(max_font_size=60,\n",
        "                      background_color='white').generate_from_frequencies(DICT_OF_WORD_FREQUENCIES)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GREtx52gXd-"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "\n",
        "The Naive Bayes Theorem:\n",
        "$P(H | E ) = \\frac{P(E | H)P(H)}{P(E)}$\n",
        "\n",
        "This probability on the left-hand side is the probability of our hypothesis given the evidence we have. In our case our hypothesis is that this film is a 'Romance Film'. Our evidence is the plot summary of the film. \n",
        "\n",
        "We can compute this probability by using the equation on the right-hand side. Let's go through what each of the terms mean:\n",
        "\n",
        "- Hypothesis = Film is a Romance film \n",
        "- Evidence = The words used in the film summary\n",
        "- P(Evidence | Hypothesis) = The probability of these words being used given the film is a Romance Film.  \n",
        "- P(Hypothesis) = $\\frac{\\text{Number of romance films}}{\\text{Total number of films}}$\n",
        "\n",
        "- P(Evidence) = The probability of these words in the summary being used.\n",
        "\n",
        "\n",
        "We're going to base these probabilities on the data we have - so for example the probability of the words occuring in the summary is going to be based on our count of how frequently the words occur in the dataset summaries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msu9WzD5gS6l"
      },
      "source": [
        "# Let's split our data into train and test sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmD85wP_Gi3X"
      },
      "source": [
        "# let's find P(love | film is not a 'Romance Film') in the train set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f1Iu-29K4Ic"
      },
      "source": [
        "# first lets recreate our method for getting a list of tokens from a string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbRuN_rQwqzb"
      },
      "source": [
        "# apply our list of tokens function to the 'Romance Film's in the train set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHEZivsmxji2"
      },
      "source": [
        "# get the count of words in the list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXE-vKxvx4Xh"
      },
      "source": [
        "# sum all of the counts for all the words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqsxT0yNx6wO"
      },
      "source": [
        "# get the count for the word love"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX1DxnHFzEP_"
      },
      "source": [
        "# divide count_love by count_all_the_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuOurf512hZl"
      },
      "source": [
        "# let's compute the P(Film is Romance | \"love\" in the summary)\n",
        "# i.e. let's compute P(\"love\" | Film is Romance)*P(Film is Romance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KoOb6oQzKb9"
      },
      "source": [
        "# now get the count of love and count of all the other words in the set of non-Romance Films\n",
        "# and compute the P(Film is not Romance | \"love\" in the summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lMjZ_K53EYd"
      },
      "source": [
        "# lets put this all together and compute all the counts for all of the words in our dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u0aORqd3JWo"
      },
      "source": [
        "# compute P(Romance Film) and P(Not Romance Film)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-UyaV5B4TQl"
      },
      "source": [
        "# compute for each word in train dataset the counts for 1. Romance Films and 2. Not Romance Films"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysu2Kp6g4bRp"
      },
      "source": [
        "# define a function that takes a summary and returns the probability of it belonging to a Romance Film and the probability of it belonging to not a Romance Film"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuvzKRUZ4mR8"
      },
      "source": [
        "# lets test our model out on a movie from the test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi8yUp8mEbKX"
      },
      "source": [
        "# Further Links and Resources\n",
        "\n",
        "## Documentation\n",
        "- [Dataset Documentation](http://www.cs.cmu.edu/~ark/personas/data/README.txt)\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/reference/index.html)\n",
        "  - [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv)\n",
        "- [WordCloud Documentation](https://amueller.github.io/word_cloud/)\n",
        "- [NLTK Documentation](https://www.nltk.org/book/)\n",
        "  - [API Docs](http://www.nltk.org/api/nltk.html)\n",
        "  - [WordNet Lemmatizer](http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer#module-nltk.stem.wordnet)\n",
        "  - [Stopwords](https://www.nltk.org/book/ch02.html)\n",
        "- [Seaborn plotting library](https://seaborn.pydata.org/tutorial.html)\n",
        "\n",
        "## Online Courses\n",
        "\n",
        "- [Learn Python @ Codecademy (free trial available)](https://www.codecademy.com/learn/learn-python-3)\n",
        "- [Data Vizualisation](https://infovis.fh-potsdam.de/tutorials/)\n",
        "- [Natural Language Processing(NLP)](https://lena-voita.github.io/nlp_course.html#main_page_content)\n",
        "- [NLP with the Spacy library](https://course.spacy.io/en/)\n",
        "- [Machine Learning](https://course.fast.ai/)\n",
        "\n",
        "## Videos\n",
        "\n",
        "- [Bayes Theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM)\n",
        "\n",
        "## Tutorials\n",
        "\n",
        "- [Pandas getting started tutorials](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html)\n",
        "- [Scikit-learn tutorials](https://scikit-learn.org/stable/getting_started.html)\n",
        "- [scikit-learn Naive Bayes explanation](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
        "- [Speech and Language Processing chapter on Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/4.pdf) "
      ]
    }
  ]
}